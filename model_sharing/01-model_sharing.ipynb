{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f38e31-cbfe-4dcb-9f97-dd57ae3a361f",
   "metadata": {},
   "source": [
    "# Model sharing\n",
    "\n",
    "## What does it mean to share a model?\n",
    "\n",
    "Model sharing can mean multiple things:\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "    root((Shared model))\n",
    "        Model weights\n",
    "            Specifies model's parameters\n",
    "        Model code\n",
    "            Specifies how the model can be run\n",
    "        Model card\n",
    "            Describes the model to users\n",
    "        Training code\n",
    "            Specifies how the model can be trained\n",
    "        Training data\n",
    "            The data used to train the model\n",
    "```\n",
    "\n",
    "Let's look at these in detail:\n",
    "\n",
    "### Model weights\n",
    "\n",
    "Model weights is the simplest to understand: it is just the parameter weights of the model.\n",
    "\n",
    "These weights are often [serialized versions](https://en.wikipedia.org/wiki/Serialization) of the numbers stored in training framework's memory. While doing training these weights are often stored by pickling Python objects ([PyTorch's serialization routines](https://docs.pytorch.org/docs/stable/notes/serialization.html) work this way), but because Python's [pickle](https://docs.python.org/3/library/pickle.html)-module is not secure, they are rarely stored in these formats when shared.\n",
    "\n",
    "Because we do not want to execute random code provided to us by strangers over the internet, multiple different formats have been designed to fix this problem.\n",
    "\n",
    "[ONNX](https://onnx.ai/) is a open standard for sharing machine learning models and [PyTorch supports is natively](https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html). It is widely used, especially in industry, where trained models do inference on various hardware devices.  \n",
    "\n",
    "ONXX converts the whole computation graph of the model into operations that can then be stored in the serialization format. Same is done for the parameters.\n",
    "\n",
    "Another popular format, especially among scientists and ML designers, is [safetensors](https://huggingface.co/docs/safetensors). This format was created by Hugging Face and it is used in many repos in Hugging Face Hub. Safetensors focuses on serializing the weights, so getting a working model from safetensors file requires access to the module structure where there parameters will be placed.\n",
    "\n",
    "### Model code\n",
    "\n",
    "Like mentioned in the previous section, sometimes getting access to the model specification is needed to construct the model.\n",
    "\n",
    "Sometimes the code is given as code in e.g. Github repositories, but sometimes the configuration is given as specification.\n",
    "\n",
    "For example, Hugging Face uses a concept called [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) that picks a model from an pre-existing list of model specifications. These models are then initialized based on [AutoConfig](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig) given in model's repository. So when you're calling `AutoModelForCausalLM.from_pretrained`, the following happens:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    R[User asks for model] --> F[\"AutoModelForCausalLM.from_pretrained(...)\"];\n",
    "    F --> H[Hugging Face Hub checks the repository];\n",
    "    H --> C[Repository contains an AutoConfig];\n",
    "    C --> M[AutoModel is initialized with layer configuration from AutoConfig];\n",
    "    M --> R\n",
    "```\n",
    "\n",
    "A good example of this is [gpt-oss-120b's model configuration](https://huggingface.co/openai/gpt-oss-120b/blob/main/config.json).\n",
    "It utilizes [GptOssForCausalLM](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_oss/modeling_gpt_oss.py#L637), which in turn is a subclass is [GptOssPreTrainedModel](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_oss/modeling_gpt_oss.py#L422), which in turn is a subclass of [PreTrainedModel](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1644).\n",
    "\n",
    "The complex model structure can be represented by a simple json-file for all GPT-like models.\n",
    "\n",
    "### Model card\n",
    "\n",
    "The idea of a machine learning model card was introduced in 2018 by [Mitchell et al.](https://arxiv.org/abs/1810.03993). The basic idea of a model card is that it should contain information on the intented use cases of the model and possible limitations of the model.\n",
    "\n",
    "Since then, the concept has been used by all of the major players in the field: [Hugging Face](https://huggingface.co/docs/hub/model-cards), [OpenAI](https://platform.openai.com/docs/models/system-cards/), [Google](https://modelcards.withgoogle.com/), [Meta](https://www.llama.com/docs/model-cards-and-prompt-formats/) to name a few.\n",
    "\n",
    "It is good to remember that writing a model card is an important piece of sharing a model.\n",
    "\n",
    "### Training code\n",
    "\n",
    "Compared to the others mentioned before, training code is not shared as often. Especially in the field of huge models like foundation large language models training can be so expensive, that keeping the training code hidden can provide a major competitive advantage to AI developers.\n",
    "\n",
    "In purely scientific fields sharing the training code is much more common and GitHub is the most common way of sharing code.\n",
    "\n",
    "### Training data\n",
    "\n",
    "Sharing training data is another complicated topic. Similar to training codes, possessing more and better quality training data will provide companies with competitive advantages and thus many of the training datasets are not shared. Questions of licensing and data ownership also limit the possibility of sharing the training data.\n",
    "\n",
    "[Hugging Face datasets](https://huggingface.co/datasets) provides lots of datasets that are commonly used for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6268bbb-09ef-4846-9831-7b5ba38a5b9e",
   "metadata": {},
   "source": [
    "## Where models are shared\n",
    "\n",
    "Models are nowadays shared through various sites, but [Hugging Face Hub](https://huggingface.co/models) is one of the most popular places for model sharing.\n",
    "\n",
    "[Zenodo](https://zenodo.org/) and other similar publicly funded storage solutions also contain datasets, but they often lose in ease of use to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0f0e9-4196-4134-992e-aabf04e7c047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-reproducibility",
   "language": "python",
   "name": "ml-reproducibility"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
