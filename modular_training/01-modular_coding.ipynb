{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57df7287-ac5e-41e8-b556-45d25f399974",
   "metadata": {},
   "source": [
    "# Modular coding when creating machine learning models\n",
    "\n",
    "## Why modularity matters?\n",
    "\n",
    "A typical machine learning model training script looks something like this:\n",
    "\n",
    "1. Start the training script from the command line\n",
    "2. Set run configuration based on command line flags or configuration files\n",
    "3. Specify data sources for training and validation\n",
    "4. Construct model based on configuration\n",
    "5. Feed data to model during training\n",
    "6. Log training information and checkpoints\n",
    "7. Save model once a stopping condition is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ca718-518d-48e6-bcc5-b1a19bf31c18",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    CL[Start the training script from the command line] --> C[Set run configuration based on command line flags or configuration files]   \n",
    "    C --> D[Specify data sources for training and validation];\n",
    "    D --> M[Construct model based on configuration];\n",
    "    M --> T[Feed data to model during training];\n",
    "    T --> L[Log training information and checkpoints];\n",
    "    L --> T;\n",
    "    L --> O[Save model once a stopping condition is reached]; \n",
    "```\n",
    "**Figure 1: Typical machine learning training script**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5d515-f398-4a65-b37d-91e67ba53e76",
   "metadata": {},
   "source": [
    "Now you could write and often many do write the whole procedure into a single script that can be read from the top to the bottom.\n",
    "\n",
    "However, this can cause serious problems with reproducibility:\n",
    "\n",
    "- Copy pasting code across multiple scripts (command line interfaces (CLI); configuration handling; training, logging and checkpointing codes)\n",
    "- Code is not shared across multiple experiments, so fixing a bug in one experiment might leave the bug in place in another\n",
    "- Combining multiple datasets is difficult as there isn't a clear idea of what kind of data your model wants \n",
    "- Data loading, model creation and training codes can mix together, which can cause problems with the frameworks\n",
    "- It is hard to keep track of different experiments\n",
    "\n",
    "## Modularity is used by every framework out there\n",
    "\n",
    "To solve the previously mentioned problems, all frameworks provide tools that make it easy to write the code in a modular form. In fact, they expect you to utilize these tools.\n",
    "\n",
    "Typically each framework has their own specification of:\n",
    "\n",
    "1. Dataset specification that specifies how to load a single data point from the stored files.\n",
    "2. Data loader that creates batches out of the dataset\n",
    "3. Model specification that defines the layers of the model and the forward pass\n",
    "4. Optimizers that specify how the model weights should be tuned\n",
    "5. Model trainer that trains the model with a chosen dataset and optimizer\n",
    "\n",
    "Many frameworks build on top of existing features like PyTorch's [torch.nn.Module](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) so they just extend the functionality of those modules.\n",
    "\n",
    "Here are some examples of these modules:\n",
    "\n",
    "| Framework | Dataset specification | Data loader | Model specification | Optimizers | Model trainer | \n",
    "| --------- | --------------------- | ----------- | ------------------- | ---------- | ------------- |\n",
    "| [PyTorch](https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html#what-is-torch-nn-really) | [torch.utils.data.Dataset](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | [torch.utils.data.DataLoader](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) | [torch.nn.Module](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) | [torch.optim](https://docs.pytorch.org/docs/stable/optim.html) |  - |\n",
    "| [HuggingFace](https://huggingface.co/docs) | [Datasets](https://huggingface.co/docs/datasets/main/en/create_dataset) | [Datasets](https://huggingface.co/docs/datasets/main/en/create_dataset) with [torch.utils.data.DataLoader](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) | [transformers.PretrainedConfig](https://huggingface.co/docs/transformers/main/en/custom_models#configuration) & [transformers.PretrainedModel](https://huggingface.co/docs/transformers/main/en/custom_models#model) (see also [huggingface_hub.PyTorchModelHubMixin](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin)) | Provided by [Trainer](https://huggingface.co/docs/transformers/main/en/optimizers) | [transformers.Trainer](https://huggingface.co/docs/transformers/trainer) |\n",
    "| [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) | [lightning.DataModule](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningDataModule.html#lightning.pytorch.core.LightningDataModule) wraps [torch.utils.data.Dataset](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | [lightning.DataModule](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningDataModule.html#lightning.pytorch.core.LightningDataModule) wraps [torch.utils.data.DataLoader](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) | [lightning.LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) wraps [torch.nn.Module](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) | - | [lightning.pytorch.Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) |\n",
    "\n",
    "The idea behind these modules is to create clear delineation between different functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf03a1-76f3-4698-bef9-0a8e6f345b13",
   "metadata": {},
   "source": [
    "### Dataset & data loader modularity\n",
    "\n",
    "```mermaid\n",
    "flowchart LR  \n",
    "    D1[Dataset 1] --> DL[Data loader];\n",
    "    D2[Dataset 2] --> DL;\n",
    "    D3[Dataset ...] --> DL;\n",
    "    DL --> B[Batch];\n",
    "```\n",
    "**Having a dataset that provides data in a consistent way makes it possible to switch data sources**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR  \n",
    "    D1[Dataset instance 1] --> DL1[Data loader process 1];\n",
    "    D2[Dataset instance 2] --> DL2[Data loader process 2];\n",
    "    D3[Dataset instance ...] --> DL3[Data loader process ...];\n",
    "    DL1 --> DC[Data loader collator];\n",
    "    DL2 --> DC;\n",
    "    DL3 --> DC;\n",
    "    DC --> B[Batch];\n",
    "```\n",
    "**When dataset provides data a single sample at a time, data loader can parallelize data loading**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR  \n",
    "    D1[Dataset] --> DL[Data loader];\n",
    "    DL --> T1[Transform process 1];\n",
    "    DL --> T2[Transform process 2];\n",
    "    DL --> T3[Transform process ...];\n",
    "    T1 --> DC[Data loader collator];\n",
    "    T2 --> DC;\n",
    "    T3 --> DC;\n",
    "    DC --> B[Batch];\n",
    "```\n",
    "**When a single dataset is used, data loading can parallelize data transforms (e.g. augmentation, normalization)**\n",
    "\n",
    "### Model modularity\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    DL[Data loader] --> T[Trainer];\n",
    "    M1[Model 1] --> T;\n",
    "    T --> O[Model 1 outputs];\n",
    "    DL2[Data loader] --> T2[Trainer];\n",
    "    M2[Model 2] --> T2;\n",
    "    T2 --> O2[Model 2 outputs];\n",
    "```\n",
    "**When model is written as a separate module, it can be modified while re-using the other parts**\n",
    "\n",
    "### Optimizer modularity\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    DL[Data loader] --> T[Trainer];\n",
    "    M[Model] --> T;\n",
    "    Op1[Optimizer 1] --> T;\n",
    "    T --> O[Model outputs with optimizer 1];\n",
    "    DL2[Data loader] --> T2[Trainer];\n",
    "    M2[Model] --> T2;\n",
    "    Op2[Optimizer 2] --> T2;\n",
    "    T2 --> O2[Model outputs with optimizer 2];\n",
    "```\n",
    "**When optimizer is written as a separate module, it can be modified while re-using the other parts**\n",
    "\n",
    "### Trainer modularity\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    DL[Data loader] --> T1[Trainer with configuration 1];\n",
    "    M[Model] --> T1;\n",
    "    Op1[Optimizer] --> T1;\n",
    "    T1 --> O[Model outputs with trainer configuration 1];\n",
    "    DL2[Data loader] --> T2[Trainer with configuration 2];\n",
    "    M2[Model] --> T2;\n",
    "    Op2[Optimizer] --> T2;\n",
    "    T2 --> O2[Model outputs with trainer configuration 2];\n",
    "```\n",
    "**When trainer is written as a separate module, it can be modified while re-using the other parts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f918107-a171-42c8-ab0a-3e1311a12ea5",
   "metadata": {},
   "source": [
    "## Modular machine learning training script\n",
    "\n",
    "So what parts are in a machine learning training script:\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "    root((Training script))\n",
    "        Dataset\n",
    "            Specifies what the data is\n",
    "        Data loader\n",
    "            Specifies how data is loaded and transformed\n",
    "        Model\n",
    "            Specifies the model strucuture\n",
    "        Optimizer\n",
    "            Specifies how the strategy how model will be trained\n",
    "        Trainer\n",
    "            Specifies how the model is trained\n",
    "        CLI & Configuration\n",
    "            Specifies command line interface and configuration reading\n",
    "```\n",
    "\n",
    "## Modular does not always mean object oriented programming\n",
    "\n",
    "Modular coding does not necessarily mean that everything needs to be written in classes. Whether you should use classes or functions depends on your preference and on the frameworks preference. Typically the datasets and model specifications are given as classes as they need to inherit features from the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b76fb-82e3-4c34-bc09-a00f57d02bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python generic (scicomp-python-env/2024-01)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
